# This script runs a set of spiders using the default compression backend.
# After running the set of spiders once, it checks their output files and
# records the size. It then swaps out the compression backend being used
# and runs the same spider again. Finally the size of the files
# the spiders create is checked again and recorded. Once this is done
# the size of the files created by the spiders during the first run
# is compared to the size of the files during the second run. The
# folowing data is then written to a file called "compresion_test.txt"
# -- The name of file output by the spider
# -- A short description of the test
# -- The size of the file under default compression
# -- The size of the file after using the new compression
# -- Percent improvment in disk usage between the two backends

import scrapy
from twisted.internet import reactor, defer
from scrapy.crawler import CrawlerRunner
from scrapy.utils.log import configure_logging
from scrapy.utils.project import get_project_settings

# To handle outputting the size of files generated by each spider
# perhaps we can use some utility functions that each spider can call
# on the file it creates after doing its work

# We will probably need a data structure to keep track of the
# the info we will write to "compression_test.txt" file that we will
# use to keep track of how much space each file uses
# I suggest a python dictionary or whatever it has

# PLACE UTILITY FUNCTIONS HERE

# END UTILITY FUNCTIONS


# ARRANGE TEST SPIDERS HERE
# Any spider used to test the compression backend should have its
# code placed here. 

# Fanfic Test Spider
# The purpose of this spider is to to test our compression algorith
# against a data set that should provide a high degree of variablity
# (the test in a bit of fanfiction. This should give us a small delta
 
class FanficSpider(scrapy.Spider):
    name = "fanfic_test"
    allowed_domains = ["fanfiction.net"]
    start_urls = [
        "https://www.fanfiction.net/s/11490724/1/Snowflake-s-Passage-First-story-Scary-Things",
        "https://www.fanfiction.net/s/11498367/1/Left-Turn"]

    def parse(self, response):
        filename = 'fanfic_test.html'
        with open(filename, 'wb') as f:
            f.write(response.body)

class OtherTestSpider(scrapy.Spider):
    name = "other_spider"
    allowed_domain = ["fanfiction.net"]
    start_urls = [
        "https://www.fanfiction.net/s/11490724/1/Snowflake-s-Passage-First-story-Scary-Things",
        "https://www.fanfiction.net/s/11498367/1/Left-Turn"]

    def parse(self, response):
        filename = 'other_test.html'
        with open(filename, 'wb') as f:
            f.write(response.body)


# END TEST SPIDERS

# Do some initial set up
configure_logging()
runner = CrawlerRunner()

@defer.inlineCallbacks
def crawl():
    # Yield the result of calling runner.crawl on your spider here
    yield runner.crawl(FanficSpider)
    yield runner.crawl(OtherTestSpider)
    reactor.stop()

crawl()
reactor.run()()
